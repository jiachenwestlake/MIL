# config-default.yaml: OpenPrompt's default configuration options


environment:
  num_gpus: 1 # number of gpus to use
  cuda_visible_devices: # which index of cuda devices is visible to the program
    - 0
  local_rank: 0 # the main device in the cuda visible devices that your DataParallel model will put the model on.
                # The following should holds: local_rank < len(cuda_visible_devices)

reproduce:  # seed for reproduction 
  seed: 100  # a seed for all random part
  random_seed: -1 # seed for random package
  torch_seed: -1 # seed for pytorch
  numpy_seed: -1 # seed for numpy 
  cuda_seed: -1 # seed for cuda

plm:   # plm parameters 
  model_name: # the model name, e.g. bert, roberta, gpt2, ...
              # for all the available model, please check the ./plms directory.
  model_path:
  specials_to_add:
    - <pad> # always need to add pad token, if the tokenizer doesn't have one.
  optimize: 
    name: AdamW  # TODO: curently not in use. 
    freeze_para: False
    no_decay:
      - bias
      - LayerNorm.weight
    lr: 0.0005
    weight_decay: 0.01
    betas: 
      - 0.9
      - 0.999
    eps: 1.0E-8
    scheduler:
      type:      # by default, it will choose get_linear_schedule_with_warmup
      num_warmup_steps: 500

## LOGGIN and CHECKPOINTING ##################################################
## in logging, each experiment will create a seperate folder for saving log.txt
## , (full) config.json, and the checkpoint (if use the same path). 
## logging is in the following formatï¼š
## ./log
##  - DIR_NAME_1
##    - log.txt
##    - config.yaml
##    - checkpoint.pt
##    - ...
##  - DIR_NAME_2
##    - ...
##
logging:
  path_base: ./logs # the path base of all the logs.
  file_level: NOTSET # make sure it's an option of logging package
  console_level: INFO # make sure it's an option of logging package
  unique_string:  # the generated (or usr defined) unique string for one experiment. 
  unique_string_keys: # used to generate the unique string for saving 
    - dataset.name
    - plm.model_path # only keep the last folder name in code, 
        # .i.e ../.cache/roberta-large/ will save as roberta-large 
    - template
    - verbalizer
    - datetime  # a 12-digit string recording the date time of running the experiment, i.e., YYMMDDHHMMSS.
  datetime_format: "%y%m%d%H%M%S" # only useful when unique_string_keys includes `datetime`.
     #  make sure it's a valid format for datetime package.
  path: # always keep none to let the config generate a full path according to 
         # path_base and unique_string.
  overwrite: True # if a same log path exists, overwrite it.

checkpoint: # checkpoint use the same directory as logging.
  save_lastest: True # Normaly set to False to reduce memory use, set
                    # to true to allow resuming learning process.
  save_best: True   # Keep saving the epoch of the best-performance. 
  higher_better: True # is the metric to determine best checkpoint highter better?


## PIPELINE #######################################################

train:
  num_epochs: 5 # the number of training epochs.
  batch_size: 2 # the batch_size.
  shuffle_data: True # whether shuffle the training data.
  teacher_forcing: False # whether perform teacher forcing in training.
                      # if true, the desired prediction on each mask will
                      # be filled in the mask.
  gradient_accumulation_steps: 1 # update weight  every N step of training.
                      # set 1 to disable gradient accumulation.
  max_grad_norm: -1.0 # <0 for unlimited gradients norm

dev:
  batch_size: 2 # evaluationn batch_size, can be a bit larger than training batch_size
  shuffle_data: False # whether to perform data shuffling in evaluation

test:
  batch_size: 2 # evaluationn batch_size, can be a bit larger than training batch_size
  shuffle_data: False # whether to perform data shuffling in evaluation
  
## TASK ##########################################################@
task: classification

classification:
  parent_config: task
  metric:  # the first one will be the main  to determine checkpoint.
    - micro-f1  # whether the higher metric value is better.
  loss_function: cross_entropy ## the loss function for classification

generation: # Adding any arguments for generation here.
  parent_config: task
  metric:
    - sentence_bleu
  max_length: 512   # the max_length of the generated sentence. INCLUDING the input_ids. So: generation.max_length > dataloader.max_seq_length
  max_new_tokens:
  min_length: 5
  temperature: 1.0
  do_sample: False
  top_k: 0
  top_p: 0.9
  repetition_penalty: 1.0 ##args.repetition_penalty,
  num_beams: 5
  bad_words_ids: 
    - 
      - 628
      - 198

  


  
  

relation_classification:
  parent_config: task

## DATASET #########################################################
dataset:
  name:   # the name of the dataset, for the supported choices,
          # please see the processors in ./data_utils/ 
  path:  # whether is the dataset saved in your local machine.

## DATALOADER ######################################################
dataloader:
  max_seq_length: 256 # max_seq_length 
  decoder_max_length: 256 # the decoder max length to truncate decoder input sequence
                    # if it is an encoder-decoder architecture. Note that it's not equavalent
                    # to generation.max_length which is used merely in the generation phase.
  truncate_method: "head" # choosing from balanced, head, tail

## LEARINING SETTING  ####################################################
learning_setting:   # selecting from "full", "zero-shot", "few-shot"

zero_shot:
  parent_config: learning_setting

few_shot:
  parent_config: learning_setting
  few_shot_sampling:
  
sampling_from_train:
  parent_config: few_shot_sampling
  num_examples_per_label: 10
  also_sample_dev: True
  num_examples_per_label_dev: 10
  seed: 123

## CALIBRATION ###########################################################
calibrate: # leave blank to use no calibrate

contextualized_calibrate:
  parent_config: calibrate
  num_example:
  use_split: train

pmi_calibrate:
  parent_config: calibrate

## PROMPT SPECIFIC CONFIG ############################################
template:
verbalizer:

manual_template:
  parent_config: template
  text: 
  mask_token: <mask>
  placeholder_mapping:
    <text_a>: text_a
    <text_b>: text_b
  file_path:
  choice: 0
  optimize:  # the parameters related to optimize the tempalte


automatic_verbalizer:
  parent_config: verbalizer
  num_cadidates: 1000
  label_word_num_per_class: 1
  num_searches: 1
  score_fct: llr
  balance: true
  optimize:
    level: epoch
  num_classes:
  init_using_split: valid

one2one_verbalizer:
  parent_config: verbalizer
  label_words:
  prefix: " "
  multi_token_handler: first
  file_path:
  choice:
  num_classes:
  optimize:
  
manual_verbalizer:
  parent_config: verbalizer
  label_words:
  prefix: " "
  multi_token_handler: first
  file_path:
  choice:
  num_classes:
  optimize:

prefix_tuning_template:
  parent_config: template
  text:
  mask_token: <mask>
  num_token: 5
  placeholder_mapping: 
    <text_a>: text_a
    <text_b>: text_b
  prefix_dropout: 0.0
  mid_dim: 512
  optimize: 
    name: AdamW  # TODO: curently not in use. 
    lr: 0.00005
    betas: 
      - 0.9
      - 0.999
    eps: 1.0E-8
    weight_decay: 0.0
    no_decay: 
      - bias
      - LayerNorm.weight
    scheduler: 
      num_warmup_steps: 0

soft_manual_template:
  choice: 0
  file_path:
  optimize: 
    name: AdamW  # TODO: curently not in use. 
    lr: 0.003
    betas: 
      - 0.9
      - 0.999
    eps: 1.0E-8
    weight_decay: 0.0
    scheduler: 
      num_warmup_steps: 0

